{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "%timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('car_augmented_file.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_url_id</th>\n",
       "      <th>is_premium</th>\n",
       "      <th>label</th>\n",
       "      <th>cars_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-xuefolan-lechi-d77BsJBbW9uE</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-xuefolan-lechi-d77BsJBbW9uE.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-fengtian-weichi-dIIJBBIhZZWZ</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-fengtian-weichi-dIIJBBIhZZWZ.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-MINI-COOPER-dB7k0BBZ9XbX</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-MINI-COOPER-dB7k0BBZ9XbX.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-aodi-A4L-dBs0iJQXuXwX</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-aodi-A4L-dBs0iJQXuXwX.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-kaidilake-CTS-d00QkBJbwwhN</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-kaidilake-CTS-d00QkBJbwwhN.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          car_url_id is_premium  label  \\\n",
       "0   2010-xuefolan-lechi-d77BsJBbW9uE      False      0   \n",
       "1  2009-fengtian-weichi-dIIJBBIhZZWZ      False      0   \n",
       "2      2014-MINI-COOPER-dB7k0BBZ9XbX       True      1   \n",
       "3         2013-aodi-A4L-dBs0iJQXuXwX       True      1   \n",
       "4    2009-kaidilake-CTS-d00QkBJbwwhN       True      1   \n",
       "\n",
       "                              cars_image  \n",
       "0   2010-xuefolan-lechi-d77BsJBbW9uE.txt  \n",
       "1  2009-fengtian-weichi-dIIJBBIhZZWZ.txt  \n",
       "2      2014-MINI-COOPER-dB7k0BBZ9XbX.txt  \n",
       "3         2013-aodi-A4L-dBs0iJQXuXwX.txt  \n",
       "4    2009-kaidilake-CTS-d00QkBJbwwhN.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the cars_image data extension from png to txt; png_ to txt_\n",
    "#because the converted alexnet data stored int .txt format instead of png format\n",
    "df['cars_image'] = df['car_url_id'].apply(lambda v: v+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_url_id</th>\n",
       "      <th>is_premium</th>\n",
       "      <th>label</th>\n",
       "      <th>cars_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-xuefolan-lechi-d77BsJBbW9uE</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-xuefolan-lechi-d77BsJBbW9uE.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-fengtian-weichi-dIIJBBIhZZWZ</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-fengtian-weichi-dIIJBBIhZZWZ.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-MINI-COOPER-dB7k0BBZ9XbX</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-MINI-COOPER-dB7k0BBZ9XbX.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-aodi-A4L-dBs0iJQXuXwX</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-aodi-A4L-dBs0iJQXuXwX.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-kaidilake-CTS-d00QkBJbwwhN</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2009-kaidilake-CTS-d00QkBJbwwhN.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          car_url_id is_premium  label  \\\n",
       "0   2010-xuefolan-lechi-d77BsJBbW9uE      False      0   \n",
       "1  2009-fengtian-weichi-dIIJBBIhZZWZ      False      0   \n",
       "2      2014-MINI-COOPER-dB7k0BBZ9XbX       True      1   \n",
       "3         2013-aodi-A4L-dBs0iJQXuXwX       True      1   \n",
       "4    2009-kaidilake-CTS-d00QkBJbwwhN       True      1   \n",
       "\n",
       "                              cars_image  \n",
       "0   2010-xuefolan-lechi-d77BsJBbW9uE.txt  \n",
       "1  2009-fengtian-weichi-dIIJBBIhZZWZ.txt  \n",
       "2      2014-MINI-COOPER-dB7k0BBZ9XbX.txt  \n",
       "3         2013-aodi-A4L-dBs0iJQXuXwX.txt  \n",
       "4    2009-kaidilake-CTS-d00QkBJbwwhN.txt  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.4 s, sys: 4.69 s, total: 38.1 s\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "extracted_alexnet = []\n",
    "\n",
    "#base on the order of the image file name, load the extracted VGG features into extracted_VGG[]\n",
    "for f in df['cars_image']:\n",
    "    \n",
    "    extracted_features = np.loadtxt('extracted_alex/{}'.format(f))\n",
    "    extracted_alexnet.append(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "\n",
    "\n",
    "onehot_labels = encoder.fit_transform(labels.reshape([-1,1]))\n",
    "onehot_labels= onehot_labels.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 4096) (1805, 4096)\n",
      "(4209, 2) (1805, 2)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset into training and testing, this should be same as previous network\n",
    "num_dat = int(len(df) * 0.7)\n",
    "\n",
    "train_dat = extracted_alexnet[:num_dat]\n",
    "train_label_one_hot = onehot_labels[:num_dat]\n",
    "\n",
    "test_dat = extracted_alexnet[num_dat:]\n",
    "test_label_one_hot = onehot_labels[num_dat:]\n",
    "\n",
    "train_dat = np.array(train_dat)\n",
    "test_dat= np.array(test_dat)\n",
    "\n",
    "#the expected alexnet converted data should be in N samples x 4096\n",
    "print(train_dat.shape, test_dat.shape)\n",
    "print(train_label_one_hot.shape, test_label_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build simple neural network tensorflow graph which accepts 4096 dimension\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import *\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 4096])\n",
    "\n",
    "W = tf.Variable(tf.zeros([4096, 2]))\n",
    "b = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "y_pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_true = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]))\n",
    "\n",
    "test_true = tf.placeholder(tf.float32, [None, 2])\n",
    "predicted_test = tf.placeholder(tf.float32, [None, 2])\n",
    "test_entropy = tf.reduce_mean(-tf.reduce_sum(test_true * tf.log(predicted_test), reduction_indices=[1]))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " #list to track the accuracy for plotting later\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "#instance to save the trained model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0\n",
      "# Train Accuracy 0.5269660251841293   Loss 0.69314593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.53      0.69      4209\n",
      "          1       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.53      0.69      4209\n",
      "\n",
      "# Test Accuracy 0.5218836565096953   Loss 0.7027619\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.52      0.69      1805\n",
      "          1       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.52      0.69      1805\n",
      "\n",
      "[[942 863]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 50\n",
      "# Train Accuracy 0.6398194345450225   Loss 1.2295539\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.91      0.51       852\n",
      "          1       0.96      0.57      0.72      3357\n",
      "\n",
      "avg / total       0.84      0.64      0.67      4209\n",
      "\n",
      "# Test Accuracy 0.6155124653739612   Loss 1.36906\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.58      0.72      1566\n",
      "          1       0.24      0.85      0.37       239\n",
      "\n",
      "avg / total       0.87      0.62      0.68      1805\n",
      "\n",
      "[[907 659]\n",
      " [ 35 204]]\n",
      "\n",
      "\n",
      "Epoch 100\n",
      "# Train Accuracy 0.6728439059158945   Loss 1.0262781\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.42      0.92      0.57      1007\n",
      "          1       0.96      0.60      0.73      3202\n",
      "\n",
      "avg / total       0.83      0.67      0.70      4209\n",
      "\n",
      "# Test Accuracy 0.6343490304709142   Loss 1.188208\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.59      0.73      1516\n",
      "          1       0.29      0.85      0.43       289\n",
      "\n",
      "avg / total       0.85      0.63      0.68      1805\n",
      "\n",
      "[[899 617]\n",
      " [ 43 246]]\n",
      "\n",
      "\n",
      "Epoch 150\n",
      "# Train Accuracy 0.6954145877880732   Loss 0.91472423\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.93      0.61      1090\n",
      "          1       0.96      0.61      0.75      3119\n",
      "\n",
      "avg / total       0.83      0.70      0.71      4209\n",
      "\n",
      "# Test Accuracy 0.6443213296398892   Loss 1.0856233\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.60      0.74      1490\n",
      "          1       0.31      0.85      0.46       315\n",
      "\n",
      "avg / total       0.84      0.64      0.69      1805\n",
      "\n",
      "[[895 595]\n",
      " [ 47 268]]\n",
      "\n",
      "\n",
      "Epoch 200\n",
      "# Train Accuracy 0.709432169161321   Loss 0.83757734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.93      0.64      1151\n",
      "          1       0.96      0.63      0.76      3058\n",
      "\n",
      "avg / total       0.83      0.71      0.72      4209\n",
      "\n",
      "# Test Accuracy 0.6548476454293629   Loss 1.017146\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.61      0.74      1461\n",
      "          1       0.34      0.85      0.48       344\n",
      "\n",
      "avg / total       0.83      0.65      0.69      1805\n",
      "\n",
      "[[890 571]\n",
      " [ 52 292]]\n",
      "\n",
      "\n",
      "Epoch 250\n",
      "# Train Accuracy 0.7234497505345688   Loss 0.7786974\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.94      0.66      1208\n",
      "          1       0.96      0.64      0.77      3001\n",
      "\n",
      "avg / total       0.83      0.72      0.74      4209\n",
      "\n",
      "# Test Accuracy 0.6731301939058172   Loss 0.96758455\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.62      0.75      1428\n",
      "          1       0.38      0.86      0.52       377\n",
      "\n",
      "avg / total       0.83      0.67      0.70      1805\n",
      "\n",
      "[[890 538]\n",
      " [ 52 325]]\n",
      "\n",
      "\n",
      "Epoch 300\n",
      "# Train Accuracy 0.7301021620337372   Loss 0.7308857\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.94      0.67      1230\n",
      "          1       0.96      0.64      0.77      2979\n",
      "\n",
      "avg / total       0.83      0.73      0.74      4209\n",
      "\n",
      "# Test Accuracy 0.6770083102493075   Loss 0.9293444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.63      0.75      1415\n",
      "          1       0.39      0.86      0.53       390\n",
      "\n",
      "avg / total       0.82      0.68      0.71      1805\n",
      "\n",
      "[[887 528]\n",
      " [ 55 335]]\n",
      "\n",
      "\n",
      "Epoch 350\n",
      "# Train Accuracy 0.7381800902827275   Loss 0.6904634\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.94      0.68      1256\n",
      "          1       0.96      0.65      0.78      2953\n",
      "\n",
      "avg / total       0.84      0.74      0.75      4209\n",
      "\n",
      "# Test Accuracy 0.6825484764542936   Loss 0.89837605\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.63      0.76      1399\n",
      "          1       0.40      0.86      0.55       406\n",
      "\n",
      "avg / total       0.82      0.68      0.71      1805\n",
      "\n",
      "[[884 515]\n",
      " [ 58 348]]\n",
      "\n",
      "\n",
      "Epoch 400\n",
      "# Train Accuracy 0.7474459491565693   Loss 0.6553308\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.95      0.70      1285\n",
      "          1       0.97      0.66      0.78      2924\n",
      "\n",
      "avg / total       0.84      0.75      0.76      4209\n",
      "\n",
      "# Test Accuracy 0.6847645429362881   Loss 0.872367\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.63      0.76      1395\n",
      "          1       0.41      0.86      0.55       410\n",
      "\n",
      "avg / total       0.82      0.68      0.71      1805\n",
      "\n",
      "[[884 511]\n",
      " [ 58 352]]\n",
      "\n",
      "\n",
      "Epoch 450\n",
      "# Train Accuracy 0.7583749109052031   Loss 0.62416625\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.96      0.71      1319\n",
      "          1       0.97      0.67      0.79      2890\n",
      "\n",
      "avg / total       0.84      0.76      0.77      4209\n",
      "\n",
      "# Test Accuracy 0.6914127423822715   Loss 0.8499072\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.64      0.76      1385\n",
      "          1       0.42      0.86      0.57       420\n",
      "\n",
      "avg / total       0.82      0.69      0.72      1805\n",
      "\n",
      "[[885 500]\n",
      " [ 57 363]]\n",
      "\n",
      "\n",
      "Epoch 500\n",
      "# Train Accuracy 0.7631266334046092   Loss 0.5960865\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.96      0.72      1339\n",
      "          1       0.97      0.67      0.79      2870\n",
      "\n",
      "avg / total       0.85      0.76      0.77      4209\n",
      "\n",
      "# Test Accuracy 0.6969529085872577   Loss 0.8300966\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.64      0.76      1367\n",
      "          1       0.44      0.86      0.58       438\n",
      "\n",
      "avg / total       0.81      0.70      0.72      1805\n",
      "\n",
      "[[881 486]\n",
      " [ 61 377]]\n",
      "\n",
      "\n",
      "Epoch 550\n",
      "# Train Accuracy 0.7702542171537182   Loss 0.57048714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.96      0.73      1363\n",
      "          1       0.97      0.68      0.80      2846\n",
      "\n",
      "avg / total       0.85      0.77      0.78      4209\n",
      "\n",
      "# Test Accuracy 0.7013850415512466   Loss 0.8123472\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.65      0.77      1359\n",
      "          1       0.45      0.86      0.59       446\n",
      "\n",
      "avg / total       0.81      0.70      0.72      1805\n",
      "\n",
      "[[881 478]\n",
      " [ 61 385]]\n",
      "\n",
      "\n",
      "Epoch 600\n",
      "# Train Accuracy 0.7790449037776194   Loss 0.54695386\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.96      0.74      1398\n",
      "          1       0.97      0.69      0.81      2811\n",
      "\n",
      "avg / total       0.85      0.78      0.79      4209\n",
      "\n",
      "# Test Accuracy 0.7019390581717452   Loss 0.79626906\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.65      0.77      1352\n",
      "          1       0.45      0.86      0.59       453\n",
      "\n",
      "avg / total       0.81      0.70      0.72      1805\n",
      "\n",
      "[[878 474]\n",
      " [ 64 389]]\n",
      "\n",
      "\n",
      "Epoch 650\n",
      "# Train Accuracy 0.7845093846519363   Loss 0.52519244\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.96      0.75      1415\n",
      "          1       0.97      0.69      0.81      2794\n",
      "\n",
      "avg / total       0.85      0.78      0.79      4209\n",
      "\n",
      "# Test Accuracy 0.7058171745152355   Loss 0.7815958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.65      0.77      1345\n",
      "          1       0.46      0.86      0.60       460\n",
      "\n",
      "avg / total       0.81      0.71      0.72      1805\n",
      "\n",
      "[[878 467]\n",
      " [ 64 396]]\n",
      "\n",
      "\n",
      "Epoch 700\n",
      "# Train Accuracy 0.7892611071513423   Loss 0.5049818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.96      0.76      1439\n",
      "          1       0.97      0.70      0.81      2770\n",
      "\n",
      "avg / total       0.85      0.79      0.79      4209\n",
      "\n",
      "# Test Accuracy 0.7063711911357341   Loss 0.76813173\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.65      0.77      1338\n",
      "          1       0.46      0.86      0.60       467\n",
      "\n",
      "avg / total       0.81      0.71      0.72      1805\n",
      "\n",
      "[[875 463]\n",
      " [ 67 400]]\n",
      "\n",
      "\n",
      "Epoch 750\n",
      "# Train Accuracy 0.7963886909004514   Loss 0.4861447\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.96      0.77      1467\n",
      "          1       0.97      0.71      0.82      2742\n",
      "\n",
      "avg / total       0.86      0.80      0.80      4209\n",
      "\n",
      "# Test Accuracy 0.7102493074792243   Loss 0.75572455\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.66      0.77      1329\n",
      "          1       0.47      0.86      0.61       476\n",
      "\n",
      "avg / total       0.81      0.71      0.73      1805\n",
      "\n",
      "[[874 455]\n",
      " [ 68 408]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 800\n",
      "# Train Accuracy 0.8037538607745308   Loss 0.46853\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.97      0.78      1496\n",
      "          1       0.97      0.71      0.82      2713\n",
      "\n",
      "avg / total       0.86      0.80      0.81      4209\n",
      "\n",
      "# Test Accuracy 0.7141274238227147   Loss 0.7442457\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.66      0.77      1322\n",
      "          1       0.48      0.86      0.62       483\n",
      "\n",
      "avg / total       0.81      0.71      0.73      1805\n",
      "\n",
      "[[874 448]\n",
      " [ 68 415]]\n",
      "\n",
      "\n",
      "Epoch 850\n",
      "# Train Accuracy 0.810168686148729   Loss 0.45200634\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.97      0.79      1521\n",
      "          1       0.97      0.72      0.83      2688\n",
      "\n",
      "avg / total       0.86      0.81      0.81      4209\n",
      "\n",
      "# Test Accuracy 0.7196675900277009   Loss 0.73358583\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.67      0.78      1308\n",
      "          1       0.49      0.86      0.63       497\n",
      "\n",
      "avg / total       0.81      0.72      0.73      1805\n",
      "\n",
      "[[872 436]\n",
      " [ 70 427]]\n",
      "\n",
      "\n",
      "Epoch 900\n",
      "# Train Accuracy 0.814920408648135   Loss 0.43645886\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.97      0.79      1541\n",
      "          1       0.97      0.73      0.83      2668\n",
      "\n",
      "avg / total       0.86      0.81      0.82      4209\n",
      "\n",
      "# Test Accuracy 0.7213296398891966   Loss 0.723649\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.67      0.78      1299\n",
      "          1       0.50      0.86      0.63       506\n",
      "\n",
      "avg / total       0.80      0.72      0.74      1805\n",
      "\n",
      "[[869 430]\n",
      " [ 73 433]]\n",
      "\n",
      "\n",
      "Epoch 950\n",
      "# Train Accuracy 0.8213352340223331   Loss 0.42178884\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.97      0.80      1564\n",
      "          1       0.98      0.73      0.84      2645\n",
      "\n",
      "avg / total       0.87      0.82      0.82      4209\n",
      "\n",
      "# Test Accuracy 0.7218836565096953   Loss 0.7143534\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.67      0.78      1292\n",
      "          1       0.51      0.85      0.64       513\n",
      "\n",
      "avg / total       0.80      0.72      0.74      1805\n",
      "\n",
      "[[866 426]\n",
      " [ 76 437]]\n",
      "\n",
      "\n",
      "Epoch 1000\n",
      "# Train Accuracy 0.8270373010216203   Loss 0.4079099\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.97      0.81      1584\n",
      "          1       0.98      0.74      0.84      2625\n",
      "\n",
      "avg / total       0.87      0.83      0.83      4209\n",
      "\n",
      "# Test Accuracy 0.7263157894736842   Loss 0.705628\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.68      0.78      1276\n",
      "          1       0.52      0.85      0.65       529\n",
      "\n",
      "avg / total       0.80      0.73      0.74      1805\n",
      "\n",
      "[[862 414]\n",
      " [ 80 449]]\n",
      "\n",
      "\n",
      "Epoch 1050\n",
      "# Train Accuracy 0.829888334521264   Loss 0.39474896\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.97      0.81      1596\n",
      "          1       0.98      0.74      0.84      2613\n",
      "\n",
      "avg / total       0.87      0.83      0.83      4209\n",
      "\n",
      "# Test Accuracy 0.7285318559556787   Loss 0.6974128\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.68      0.78      1266\n",
      "          1       0.53      0.85      0.65       539\n",
      "\n",
      "avg / total       0.80      0.73      0.74      1805\n",
      "\n",
      "[[859 407]\n",
      " [ 83 456]]\n",
      "\n",
      "\n",
      "Epoch 1100\n",
      "# Train Accuracy 0.83464005702067   Loss 0.38224348\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.97      0.82      1612\n",
      "          1       0.98      0.75      0.85      2597\n",
      "\n",
      "avg / total       0.87      0.83      0.84      4209\n",
      "\n",
      "# Test Accuracy 0.7307479224376732   Loss 0.68965703\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.68      0.78      1262\n",
      "          1       0.53      0.85      0.65       543\n",
      "\n",
      "avg / total       0.80      0.73      0.74      1805\n",
      "\n",
      "[[859 403]\n",
      " [ 83 460]]\n",
      "\n",
      "\n",
      "Epoch 1150\n",
      "# Train Accuracy 0.8398669517700166   Loss 0.37033966\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.97      0.82      1630\n",
      "          1       0.98      0.76      0.85      2579\n",
      "\n",
      "avg / total       0.88      0.84      0.84      4209\n",
      "\n",
      "# Test Accuracy 0.7301939058171745   Loss 0.6823183\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.68      0.78      1259\n",
      "          1       0.53      0.84      0.65       546\n",
      "\n",
      "avg / total       0.80      0.73      0.74      1805\n",
      "\n",
      "[[857 402]\n",
      " [ 85 461]]\n",
      "\n",
      "\n",
      "Epoch 1200\n",
      "# Train Accuracy 0.8443810881444523   Loss 0.35899287\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.97      0.83      1651\n",
      "          1       0.98      0.76      0.86      2558\n",
      "\n",
      "avg / total       0.88      0.84      0.85      4209\n",
      "\n",
      "# Test Accuracy 0.7301939058171745   Loss 0.6753611\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.68      0.78      1255\n",
      "          1       0.54      0.84      0.66       550\n",
      "\n",
      "avg / total       0.79      0.73      0.74      1805\n",
      "\n",
      "[[855 400]\n",
      " [ 87 463]]\n",
      "\n",
      "\n",
      "Epoch 1250\n",
      "# Train Accuracy 0.8484200522689475   Loss 0.34816384\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.97      0.84      1664\n",
      "          1       0.98      0.77      0.86      2545\n",
      "\n",
      "avg / total       0.88      0.85      0.85      4209\n",
      "\n",
      "# Test Accuracy 0.7318559556786703   Loss 0.668756\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.68      0.78      1248\n",
      "          1       0.54      0.84      0.66       557\n",
      "\n",
      "avg / total       0.79      0.73      0.74      1805\n",
      "\n",
      "[[853 395]\n",
      " [ 89 468]]\n",
      "\n",
      "\n",
      "Epoch 1300\n",
      "# Train Accuracy 0.8524590163934426   Loss 0.3378192\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.98      0.84      1681\n",
      "          1       0.98      0.77      0.86      2528\n",
      "\n",
      "avg / total       0.88      0.85      0.85      4209\n",
      "\n",
      "# Test Accuracy 0.7329639889196676   Loss 0.6624785\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78      1242\n",
      "          1       0.55      0.84      0.66       563\n",
      "\n",
      "avg / total       0.79      0.73      0.74      1805\n",
      "\n",
      "[[851 391]\n",
      " [ 91 472]]\n",
      "\n",
      "\n",
      "Epoch 1350\n",
      "# Train Accuracy 0.8562603943929674   Loss 0.32792944\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.98      0.85      1695\n",
      "          1       0.98      0.78      0.87      2514\n",
      "\n",
      "avg / total       0.89      0.86      0.86      4209\n",
      "\n",
      "# Test Accuracy 0.7324099722991689   Loss 0.6565073\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78      1235\n",
      "          1       0.55      0.83      0.66       570\n",
      "\n",
      "avg / total       0.79      0.73      0.74      1805\n",
      "\n",
      "[[847 388]\n",
      " [ 95 475]]\n",
      "\n",
      "\n",
      "Epoch 1400\n",
      "# Train Accuracy 0.859824186267522   Loss 0.31846774\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.98      0.85      1708\n",
      "          1       0.98      0.78      0.87      2501\n",
      "\n",
      "avg / total       0.89      0.86      0.86      4209\n",
      "\n",
      "# Test Accuracy 0.7324099722991689   Loss 0.6508242\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78      1231\n",
      "          1       0.55      0.83      0.66       574\n",
      "\n",
      "avg / total       0.79      0.73      0.74      1805\n",
      "\n",
      "[[845 386]\n",
      " [ 97 477]]\n",
      "\n",
      "\n",
      "Epoch 1450\n",
      "# Train Accuracy 0.864575908766928   Loss 0.30941054\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.98      0.86      1724\n",
      "          1       0.98      0.79      0.87      2485\n",
      "\n",
      "avg / total       0.89      0.86      0.87      4209\n",
      "\n",
      "# Test Accuracy 0.7346260387811634   Loss 0.64541346\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78      1227\n",
      "          1       0.56      0.83      0.67       578\n",
      "\n",
      "avg / total       0.79      0.73      0.74      1805\n",
      "\n",
      "[[845 382]\n",
      " [ 97 481]]\n",
      "\n",
      "\n",
      "Epoch 1500\n",
      "# Train Accuracy 0.867664528391542   Loss 0.30073467\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.98      0.86      1739\n",
      "          1       0.98      0.79      0.88      2470\n",
      "\n",
      "avg / total       0.89      0.87      0.87      4209\n",
      "\n",
      "# Test Accuracy 0.7357340720221607   Loss 0.6402602\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78      1223\n",
      "          1       0.56      0.83      0.67       582\n",
      "\n",
      "avg / total       0.79      0.74      0.74      1805\n",
      "\n",
      "[[844 379]\n",
      " [ 98 484]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1550\n",
      "# Train Accuracy 0.8702779757662152   Loss 0.2924195\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.98      0.86      1750\n",
      "          1       0.98      0.79      0.88      2459\n",
      "\n",
      "avg / total       0.89      0.87      0.87      4209\n",
      "\n",
      "# Test Accuracy 0.7351800554016621   Loss 0.63535106\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.69      0.78      1218\n",
      "          1       0.56      0.83      0.67       587\n",
      "\n",
      "avg / total       0.79      0.74      0.74      1805\n",
      "\n",
      "[[841 377]\n",
      " [101 486]]\n",
      "\n",
      "\n",
      "Epoch 1600\n",
      "# Train Accuracy 0.874792112140651   Loss 0.28444394\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.98      0.87      1769\n",
      "          1       0.98      0.80      0.88      2440\n",
      "\n",
      "avg / total       0.90      0.87      0.88      4209\n",
      "\n",
      "# Test Accuracy 0.7373961218836566   Loss 0.630673\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.69      0.78      1210\n",
      "          1       0.57      0.83      0.67       595\n",
      "\n",
      "avg / total       0.78      0.74      0.75      1805\n",
      "\n",
      "[[839 371]\n",
      " [103 492]]\n",
      "\n",
      "\n",
      "Epoch 1650\n",
      "# Train Accuracy 0.8776431456402946   Loss 0.27678922\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.98      0.87      1781\n",
      "          1       0.98      0.80      0.88      2428\n",
      "\n",
      "avg / total       0.90      0.88      0.88      4209\n",
      "\n",
      "# Test Accuracy 0.7401662049861496   Loss 0.626213\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.70      0.78      1199\n",
      "          1       0.58      0.83      0.68       606\n",
      "\n",
      "avg / total       0.78      0.74      0.75      1805\n",
      "\n",
      "[[836 363]\n",
      " [106 500]]\n",
      "\n",
      "\n",
      "Epoch 1700\n",
      "# Train Accuracy 0.8804941791399382   Loss 0.2694383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.98      0.87      1791\n",
      "          1       0.98      0.81      0.89      2418\n",
      "\n",
      "avg / total       0.90      0.88      0.88      4209\n",
      "\n",
      "# Test Accuracy 0.7418282548476455   Loss 0.6219615\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.70      0.78      1194\n",
      "          1       0.58      0.82      0.68       611\n",
      "\n",
      "avg / total       0.78      0.74      0.75      1805\n",
      "\n",
      "[[835 359]\n",
      " [107 504]]\n",
      "\n",
      "\n",
      "Epoch 1750\n",
      "# Train Accuracy 0.8823948681397007   Loss 0.26237231\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.98      0.88      1801\n",
      "          1       0.98      0.81      0.89      2408\n",
      "\n",
      "avg / total       0.90      0.88      0.88      4209\n",
      "\n",
      "# Test Accuracy 0.7407202216066482   Loss 0.61790556\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.70      0.78      1190\n",
      "          1       0.59      0.82      0.68       615\n",
      "\n",
      "avg / total       0.78      0.74      0.75      1805\n",
      "\n",
      "[[832 358]\n",
      " [110 505]]\n",
      "\n",
      "\n",
      "Epoch 1800\n",
      "# Train Accuracy 0.8869090045141363   Loss 0.25557443\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.98      0.88      1822\n",
      "          1       0.98      0.82      0.89      2387\n",
      "\n",
      "avg / total       0.90      0.89      0.89      4209\n",
      "\n",
      "# Test Accuracy 0.7423822714681441   Loss 0.6140337\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.70      0.78      1183\n",
      "          1       0.59      0.82      0.69       622\n",
      "\n",
      "avg / total       0.78      0.74      0.75      1805\n",
      "\n",
      "[[830 353]\n",
      " [112 510]]\n",
      "\n",
      "\n",
      "Epoch 1850\n",
      "# Train Accuracy 0.8895224518888097   Loss 0.24903205\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.98      0.89      1835\n",
      "          1       0.98      0.82      0.89      2374\n",
      "\n",
      "avg / total       0.91      0.89      0.89      4209\n",
      "\n",
      "# Test Accuracy 0.7423822714681441   Loss 0.61033905\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.70      0.78      1175\n",
      "          1       0.60      0.82      0.69       630\n",
      "\n",
      "avg / total       0.78      0.74      0.75      1805\n",
      "\n",
      "[[826 349]\n",
      " [116 514]]\n",
      "\n",
      "\n",
      "Epoch 1900\n",
      "# Train Accuracy 0.8940365882632454   Loss 0.24272627\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89      1854\n",
      "          1       0.98      0.83      0.90      2355\n",
      "\n",
      "avg / total       0.91      0.89      0.89      4209\n",
      "\n",
      "# Test Accuracy 0.746814404432133   Loss 0.606807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.71      0.78      1167\n",
      "          1       0.60      0.82      0.70       638\n",
      "\n",
      "avg / total       0.78      0.75      0.75      1805\n",
      "\n",
      "[[826 341]\n",
      " [116 522]]\n",
      "\n",
      "\n",
      "Epoch 1950\n",
      "# Train Accuracy 0.8976003801378   Loss 0.23664726\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89      1869\n",
      "          1       0.98      0.83      0.90      2340\n",
      "\n",
      "avg / total       0.91      0.90      0.90      4209\n",
      "\n",
      "# Test Accuracy 0.7473684210526316   Loss 0.6034322\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.71      0.78      1162\n",
      "          1       0.61      0.82      0.70       643\n",
      "\n",
      "avg / total       0.78      0.75      0.75      1805\n",
      "\n",
      "[[824 338]\n",
      " [118 525]]\n",
      "\n",
      "\n",
      "Epoch 2000\n",
      "# Train Accuracy 0.9016393442622951   Loss 0.23077999\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.98      0.90      1884\n",
      "          1       0.98      0.84      0.90      2325\n",
      "\n",
      "avg / total       0.91      0.90      0.90      4209\n",
      "\n",
      "# Test Accuracy 0.749584487534626   Loss 0.6002035\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.71      0.78      1154\n",
      "          1       0.62      0.82      0.70       651\n",
      "\n",
      "avg / total       0.78      0.75      0.75      1805\n",
      "\n",
      "[[822 332]\n",
      " [120 531]]\n",
      "\n",
      "\n",
      "Epoch 2050\n",
      "# Train Accuracy 0.9066286528866714   Loss 0.22511315\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.98      0.90      1903\n",
      "          1       0.98      0.85      0.91      2306\n",
      "\n",
      "avg / total       0.92      0.91      0.91      4209\n",
      "\n",
      "# Test Accuracy 0.7512465373961219   Loss 0.5971128\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.71      0.79      1149\n",
      "          1       0.62      0.82      0.70       656\n",
      "\n",
      "avg / total       0.78      0.75      0.76      1805\n",
      "\n",
      "[[821 328]\n",
      " [121 535]]\n",
      "\n",
      "\n",
      "Epoch 2100\n",
      "# Train Accuracy 0.9106676170111665   Loss 0.21963616\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.98      0.91      1920\n",
      "          1       0.98      0.85      0.91      2289\n",
      "\n",
      "avg / total       0.92      0.91      0.91      4209\n",
      "\n",
      "# Test Accuracy 0.754016620498615   Loss 0.5941528\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.72      0.79      1140\n",
      "          1       0.63      0.82      0.71       665\n",
      "\n",
      "avg / total       0.78      0.75      0.76      1805\n",
      "\n",
      "[[819 321]\n",
      " [123 542]]\n",
      "\n",
      "\n",
      "Epoch 2150\n",
      "# Train Accuracy 0.9142314088857211   Loss 0.21433784\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.98      0.91      1933\n",
      "          1       0.98      0.86      0.92      2276\n",
      "\n",
      "avg / total       0.92      0.91      0.91      4209\n",
      "\n",
      "# Test Accuracy 0.7551246537396122   Loss 0.5913147\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.72      0.79      1134\n",
      "          1       0.63      0.81      0.71       671\n",
      "\n",
      "avg / total       0.78      0.76      0.76      1805\n",
      "\n",
      "[[817 317]\n",
      " [125 546]]\n",
      "\n",
      "\n",
      "Epoch 2200\n",
      "# Train Accuracy 0.9170824423853647   Loss 0.20920986\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.98      0.92      1945\n",
      "          1       0.98      0.86      0.92      2264\n",
      "\n",
      "avg / total       0.92      0.92      0.92      4209\n",
      "\n",
      "# Test Accuracy 0.7573407202216067   Loss 0.5885926\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.72      0.79      1126\n",
      "          1       0.64      0.81      0.72       679\n",
      "\n",
      "avg / total       0.78      0.76      0.76      1805\n",
      "\n",
      "[[815 311]\n",
      " [127 552]]\n",
      "\n",
      "\n",
      "Epoch 2250\n",
      "# Train Accuracy 0.9194583036350678   Loss 0.20424332\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.98      0.92      1957\n",
      "          1       0.98      0.87      0.92      2252\n",
      "\n",
      "avg / total       0.93      0.92      0.92      4209\n",
      "\n",
      "# Test Accuracy 0.7584487534626039   Loss 0.58597964\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.73      0.79      1124\n",
      "          1       0.64      0.81      0.72       681\n",
      "\n",
      "avg / total       0.78      0.76      0.76      1805\n",
      "\n",
      "[[815 309]\n",
      " [127 554]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 2300\n",
      "# Train Accuracy 0.9223093371347113   Loss 0.1994303\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.98      0.92      1967\n",
      "          1       0.98      0.87      0.92      2242\n",
      "\n",
      "avg / total       0.93      0.92      0.92      4209\n",
      "\n",
      "# Test Accuracy 0.7623268698060942   Loss 0.5834695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.73      0.79      1115\n",
      "          1       0.65      0.81      0.72       690\n",
      "\n",
      "avg / total       0.78      0.76      0.77      1805\n",
      "\n",
      "[[814 301]\n",
      " [128 562]]\n",
      "\n",
      "\n",
      "Epoch 2350\n",
      "# Train Accuracy 0.9261107151342362   Loss 0.19476467\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.98      0.93      1983\n",
      "          1       0.98      0.88      0.93      2226\n",
      "\n",
      "avg / total       0.93      0.93      0.93      4209\n",
      "\n",
      "# Test Accuracy 0.7634349030470914   Loss 0.5810578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.73      0.79      1107\n",
      "          1       0.66      0.81      0.73       698\n",
      "\n",
      "avg / total       0.78      0.76      0.77      1805\n",
      "\n",
      "[[811 296]\n",
      " [131 567]]\n",
      "\n",
      "\n",
      "Epoch 2400\n",
      "# Train Accuracy 0.9284865763839392   Loss 0.19023958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.98      0.93      1999\n",
      "          1       0.98      0.88      0.93      2210\n",
      "\n",
      "avg / total       0.93      0.93      0.93      4209\n",
      "\n",
      "# Test Accuracy 0.7639889196675901   Loss 0.5787382\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.73      0.79      1102\n",
      "          1       0.66      0.81      0.73       703\n",
      "\n",
      "avg / total       0.78      0.76      0.77      1805\n",
      "\n",
      "[[809 293]\n",
      " [133 570]]\n",
      "\n",
      "\n",
      "Epoch 2450\n",
      "# Train Accuracy 0.9334758850083155   Loss 0.18585096\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.98      0.93      2022\n",
      "          1       0.98      0.89      0.93      2187\n",
      "\n",
      "avg / total       0.94      0.93      0.93      4209\n",
      "\n",
      "# Test Accuracy 0.7645429362880887   Loss 0.57650805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.74      0.79      1095\n",
      "          1       0.67      0.81      0.73       710\n",
      "\n",
      "avg / total       0.78      0.76      0.77      1805\n",
      "\n",
      "[[806 289]\n",
      " [136 574]]\n",
      "\n",
      "\n",
      "Epoch 2500\n",
      "# Train Accuracy 0.9353765740080779   Loss 0.18159398\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      2032\n",
      "          1       0.98      0.89      0.93      2177\n",
      "\n",
      "avg / total       0.94      0.94      0.94      4209\n",
      "\n",
      "# Test Accuracy 0.7639889196675901   Loss 0.57436323\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.74      0.79      1092\n",
      "          1       0.67      0.81      0.73       713\n",
      "\n",
      "avg / total       0.78      0.76      0.77      1805\n",
      "\n",
      "[[804 288]\n",
      " [138 575]]\n",
      "\n",
      "\n",
      "Epoch 2550\n",
      "# Train Accuracy 0.9382276075077215   Loss 0.17746596\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94      2046\n",
      "          1       0.98      0.90      0.94      2163\n",
      "\n",
      "avg / total       0.94      0.94      0.94      4209\n",
      "\n",
      "# Test Accuracy 0.7628808864265928   Loss 0.5723009\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.74      0.79      1084\n",
      "          1       0.67      0.80      0.73       721\n",
      "\n",
      "avg / total       0.78      0.76      0.77      1805\n",
      "\n",
      "[[799 285]\n",
      " [143 578]]\n",
      "\n",
      "\n",
      "Epoch 2600\n",
      "# Train Accuracy 0.9396531242575433   Loss 0.17346558\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.98      0.94      2056\n",
      "          1       0.98      0.90      0.94      2153\n",
      "\n",
      "avg / total       0.94      0.94      0.94      4209\n",
      "\n",
      "# Test Accuracy 0.7639889196675901   Loss 0.57032067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.74      0.79      1076\n",
      "          1       0.68      0.80      0.73       729\n",
      "\n",
      "avg / total       0.78      0.76      0.77      1805\n",
      "\n",
      "[[796 280]\n",
      " [146 583]]\n",
      "\n",
      "\n",
      "Epoch 2650\n",
      "# Train Accuracy 0.942504157757187   Loss 0.16959296\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.98      0.94      2068\n",
      "          1       0.98      0.91      0.94      2141\n",
      "\n",
      "avg / total       0.94      0.94      0.94      4209\n",
      "\n",
      "# Test Accuracy 0.7667590027700831   Loss 0.5684221\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.75      0.79      1061\n",
      "          1       0.69      0.80      0.74       744\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[791 270]\n",
      " [151 593]]\n",
      "\n",
      "\n",
      "Epoch 2700\n",
      "# Train Accuracy 0.9453551912568307   Loss 0.16585125\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95      2080\n",
      "          1       0.98      0.91      0.94      2129\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4209\n",
      "\n",
      "# Test Accuracy 0.7678670360110803   Loss 0.5666081\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.75      0.79      1057\n",
      "          1       0.69      0.80      0.74       748\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[790 267]\n",
      " [152 596]]\n",
      "\n",
      "\n",
      "Epoch 2750\n",
      "# Train Accuracy 0.9453551912568307   Loss 0.162247\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95      2084\n",
      "          1       0.98      0.91      0.94      2125\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4209\n",
      "\n",
      "# Test Accuracy 0.7684210526315789   Loss 0.5648846\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.75      0.79      1052\n",
      "          1       0.69      0.80      0.74       753\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[788 264]\n",
      " [154 599]]\n",
      "\n",
      "\n",
      "Epoch 2800\n",
      "# Train Accuracy 0.947255880256593   Loss 0.15879196\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95      2100\n",
      "          1       0.97      0.92      0.95      2109\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4209\n",
      "\n",
      "# Test Accuracy 0.7684210526315789   Loss 0.56326336\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.75      0.79      1044\n",
      "          1       0.70      0.79      0.74       761\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[784 260]\n",
      " [158 603]]\n",
      "\n",
      "\n",
      "Epoch 2850\n",
      "# Train Accuracy 0.949631741506296   Loss 0.15550552\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.97      0.95      2118\n",
      "          1       0.97      0.93      0.95      2091\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4209\n",
      "\n",
      "# Test Accuracy 0.7684210526315789   Loss 0.5617633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.75      0.79      1036\n",
      "          1       0.70      0.79      0.74       769\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[780 256]\n",
      " [162 607]]\n",
      "\n",
      "\n",
      "Epoch 2900\n",
      "# Train Accuracy 0.9524827750059397   Loss 0.15241727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95      2134\n",
      "          1       0.97      0.93      0.95      2075\n",
      "\n",
      "avg / total       0.95      0.95      0.95      4209\n",
      "\n",
      "# Test Accuracy 0.7678670360110803   Loss 0.560415\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.76      0.79      1025\n",
      "          1       0.71      0.78      0.74       780\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[774 251]\n",
      " [168 612]]\n",
      "\n",
      "\n",
      "Epoch 2950\n",
      "# Train Accuracy 0.9548586362556427   Loss 0.1495685\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.96      2150\n",
      "          1       0.97      0.94      0.95      2059\n",
      "\n",
      "avg / total       0.96      0.95      0.95      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.55926335\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.76      0.79      1017\n",
      "          1       0.72      0.78      0.75       788\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[772 245]\n",
      " [170 618]]\n",
      "\n",
      "\n",
      "Epoch 3000\n",
      "# Train Accuracy 0.9593727726300784   Loss 0.14701138\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.97      0.96      2175\n",
      "          1       0.97      0.95      0.96      2034\n",
      "\n",
      "avg / total       0.96      0.96      0.96      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.55836815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.76      0.79      1007\n",
      "          1       0.72      0.78      0.75       798\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[768 239]\n",
      " [174 624]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 3050\n",
      "# Train Accuracy 0.9610358755048705   Loss 0.14479731\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.96      2188\n",
      "          1       0.97      0.95      0.96      2021\n",
      "\n",
      "avg / total       0.96      0.96      0.96      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.5577965\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.76      0.78       997\n",
      "          1       0.73      0.78      0.75       808\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[761 236]\n",
      " [181 627]]\n",
      "\n",
      "\n",
      "Epoch 3100\n",
      "# Train Accuracy 0.9612734616298408   Loss 0.14295404\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.96      2201\n",
      "          1       0.96      0.96      0.96      2008\n",
      "\n",
      "avg / total       0.96      0.96      0.96      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5576015\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.77      0.79       986\n",
      "          1       0.74      0.78      0.76       819\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[761 225]\n",
      " [181 638]]\n",
      "\n",
      "\n",
      "Epoch 3150\n",
      "# Train Accuracy 0.9636493228795439   Loss 0.14145581\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.97      2215\n",
      "          1       0.96      0.96      0.96      1994\n",
      "\n",
      "avg / total       0.96      0.96      0.96      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.5577916\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       977\n",
      "          1       0.74      0.78      0.76       828\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[756 221]\n",
      " [186 642]]\n",
      "\n",
      "\n",
      "Epoch 3200\n",
      "# Train Accuracy 0.9648372535043953   Loss 0.1402169\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2220\n",
      "          1       0.96      0.96      0.96      1989\n",
      "\n",
      "avg / total       0.96      0.96      0.96      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.55831313\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       971\n",
      "          1       0.75      0.77      0.76       834\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[754 217]\n",
      " [188 646]]\n",
      "\n",
      "\n",
      "Epoch 3250\n",
      "# Train Accuracy 0.9648372535043953   Loss 0.13912828\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97      2226\n",
      "          1       0.96      0.96      0.96      1983\n",
      "\n",
      "avg / total       0.96      0.96      0.96      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5590713\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       970\n",
      "          1       0.75      0.77      0.76       835\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 217]\n",
      " [189 646]]\n",
      "\n",
      "\n",
      "Epoch 3300\n",
      "# Train Accuracy 0.965312425754336   Loss 0.13810854\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2226\n",
      "          1       0.96      0.97      0.96      1983\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.55997235\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       970\n",
      "          1       0.75      0.77      0.76       835\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 217]\n",
      " [189 646]]\n",
      "\n",
      "\n",
      "Epoch 3350\n",
      "# Train Accuracy 0.9657875980042766   Loss 0.13712011\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2228\n",
      "          1       0.96      0.97      0.96      1981\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5609496\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 216]\n",
      " [190 647]]\n",
      "\n",
      "\n",
      "Epoch 3400\n",
      "# Train Accuracy 0.9662627702542171   Loss 0.13615172\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2230\n",
      "          1       0.96      0.97      0.96      1979\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7739612188365651   Loss 0.56196475\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[751 217]\n",
      " [191 646]]\n",
      "\n",
      "\n",
      "Epoch 3450\n",
      "# Train Accuracy 0.9672131147540983   Loss 0.13520065\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2228\n",
      "          1       0.96      0.97      0.97      1981\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.5629984\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 3500\n",
      "# Train Accuracy 0.9679258731290092   Loss 0.13426605\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2227\n",
      "          1       0.96      0.97      0.97      1982\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.56404287\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 3550\n",
      "# Train Accuracy 0.9688762176288904   Loss 0.13334712\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2227\n",
      "          1       0.96      0.97      0.97      1982\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.56509477\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 3600\n",
      "# Train Accuracy 0.9691138037538608   Loss 0.13244325\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2228\n",
      "          1       0.96      0.97      0.97      1981\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.5661534\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       965\n",
      "          1       0.75      0.77      0.76       840\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[750 215]\n",
      " [192 648]]\n",
      "\n",
      "\n",
      "Epoch 3650\n",
      "# Train Accuracy 0.9691138037538608   Loss 0.13155387\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2228\n",
      "          1       0.96      0.97      0.97      1981\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5672175\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       964\n",
      "          1       0.75      0.77      0.76       841\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[750 214]\n",
      " [192 649]]\n",
      "\n",
      "\n",
      "Epoch 3700\n",
      "# Train Accuracy 0.9693513898788311   Loss 0.13067843\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2229\n",
      "          1       0.96      0.97      0.97      1980\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5682867\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       964\n",
      "          1       0.75      0.77      0.76       841\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[750 214]\n",
      " [192 649]]\n",
      "\n",
      "\n",
      "Epoch 3750\n",
      "# Train Accuracy 0.970064148253742   Loss 0.12981647\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2228\n",
      "          1       0.97      0.97      0.97      1981\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.56936055\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       964\n",
      "          1       0.75      0.77      0.76       841\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[750 214]\n",
      " [192 649]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 3800\n",
      "# Train Accuracy 0.9710144927536232   Loss 0.12896763\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2226\n",
      "          1       0.97      0.97      0.97      1983\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5704385\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       964\n",
      "          1       0.75      0.77      0.76       841\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[750 214]\n",
      " [192 649]]\n",
      "\n",
      "\n",
      "Epoch 3850\n",
      "# Train Accuracy 0.9719648372535044   Loss 0.12813136\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2224\n",
      "          1       0.97      0.97      0.97      1985\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.57152003\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       964\n",
      "          1       0.75      0.77      0.76       841\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[750 214]\n",
      " [192 649]]\n",
      "\n",
      "\n",
      "Epoch 3900\n",
      "# Train Accuracy 0.9722024233784747   Loss 0.12730731\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2223\n",
      "          1       0.97      0.97      0.97      1986\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5726048\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[751 215]\n",
      " [191 648]]\n",
      "\n",
      "\n",
      "Epoch 3950\n",
      "# Train Accuracy 0.9736279401282965   Loss 0.12649518\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.98      2223\n",
      "          1       0.97      0.97      0.97      1986\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5736923\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[751 215]\n",
      " [191 648]]\n",
      "\n",
      "\n",
      "Epoch 4000\n",
      "# Train Accuracy 0.9743406985032074   Loss 0.12569453\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.98      2222\n",
      "          1       0.97      0.97      0.97      1987\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5747823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[751 215]\n",
      " [191 648]]\n",
      "\n",
      "\n",
      "Epoch 4050\n",
      "# Train Accuracy 0.9745782846281777   Loss 0.12490507\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.98      2223\n",
      "          1       0.97      0.97      0.97      1986\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5758745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[751 215]\n",
      " [191 648]]\n",
      "\n",
      "\n",
      "Epoch 4100\n",
      "# Train Accuracy 0.974815870753148   Loss 0.12412661\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2222\n",
      "          1       0.97      0.97      0.97      1987\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.57696855\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 216]\n",
      " [190 647]]\n",
      "\n",
      "\n",
      "Epoch 4150\n",
      "# Train Accuracy 0.9755286291280589   Loss 0.123358704\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2223\n",
      "          1       0.97      0.98      0.97      1986\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5780642\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 216]\n",
      " [190 647]]\n",
      "\n",
      "\n",
      "Epoch 4200\n",
      "# Train Accuracy 0.9755286291280589   Loss 0.12260118\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2223\n",
      "          1       0.97      0.98      0.97      1986\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.57916105\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 216]\n",
      " [190 647]]\n",
      "\n",
      "\n",
      "Epoch 4250\n",
      "# Train Accuracy 0.9762413875029698   Loss 0.12185377\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2224\n",
      "          1       0.97      0.98      0.97      1985\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.5802591\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 4300\n",
      "# Train Accuracy 0.9767165597529104   Loss 0.1211162\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2224\n",
      "          1       0.97      0.98      0.98      1985\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.5813581\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 4350\n",
      "# Train Accuracy 0.9769541458778808   Loss 0.1203882\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2225\n",
      "          1       0.97      0.98      0.98      1984\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.58245754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 216]\n",
      " [190 647]]\n",
      "\n",
      "\n",
      "Epoch 4400\n",
      "# Train Accuracy 0.977191732002851   Loss 0.1196696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2224\n",
      "          1       0.97      0.98      0.98      1985\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5835575\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 216]\n",
      " [190 647]]\n",
      "\n",
      "\n",
      "Epoch 4450\n",
      "# Train Accuracy 0.977191732002851   Loss 0.11896018\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2224\n",
      "          1       0.97      0.98      0.98      1985\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5846577\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 216]\n",
      " [190 647]]\n",
      "\n",
      "\n",
      "Epoch 4500\n",
      "# Train Accuracy 0.977904490377762   Loss 0.11825974\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2223\n",
      "          1       0.98      0.98      0.98      1986\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.5857581\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 215]\n",
      " [190 648]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 4550\n",
      "# Train Accuracy 0.9781420765027322   Loss 0.11756808\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2222\n",
      "          1       0.98      0.98      0.98      1987\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7767313019390581   Loss 0.58685815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 214]\n",
      " [189 649]]\n",
      "\n",
      "\n",
      "Epoch 4600\n",
      "# Train Accuracy 0.9786172487526729   Loss 0.11688497\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7767313019390581   Loss 0.587958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 214]\n",
      " [189 649]]\n",
      "\n",
      "\n",
      "Epoch 4650\n",
      "# Train Accuracy 0.9788548348776431   Loss 0.11621026\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2221\n",
      "          1       0.98      0.98      0.98      1988\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7761772853185596   Loss 0.5890576\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 214]\n",
      " [190 649]]\n",
      "\n",
      "\n",
      "Epoch 4700\n",
      "# Train Accuracy 0.9790924210026134   Loss 0.11554378\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7767313019390581   Loss 0.5901566\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 214]\n",
      " [189 649]]\n",
      "\n",
      "\n",
      "Epoch 4750\n",
      "# Train Accuracy 0.9790924210026134   Loss 0.11488533\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7761772853185596   Loss 0.5912549\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 215]\n",
      " [189 648]]\n",
      "\n",
      "\n",
      "Epoch 4800\n",
      "# Train Accuracy 0.9790924210026134   Loss 0.11423479\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7761772853185596   Loss 0.59235245\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 215]\n",
      " [189 648]]\n",
      "\n",
      "\n",
      "Epoch 4850\n",
      "# Train Accuracy 0.9798051793775243   Loss 0.11359194\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.5934491\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 216]\n",
      " [189 647]]\n",
      "\n",
      "\n",
      "Epoch 4900\n",
      "# Train Accuracy 0.9798051793775243   Loss 0.11295673\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.59454465\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 216]\n",
      " [189 647]]\n",
      "\n",
      "\n",
      "Epoch 4950\n",
      "# Train Accuracy 0.9798051793775243   Loss 0.11232888\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.59563917\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[752 217]\n",
      " [190 646]]\n",
      "\n",
      "\n",
      "Epoch 5000\n",
      "# Train Accuracy 0.980280351627465   Loss 0.11170833\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.5967325\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       970\n",
      "          1       0.75      0.77      0.76       835\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 217]\n",
      " [189 646]]\n",
      "\n",
      "\n",
      "Epoch 5050\n",
      "# Train Accuracy 0.9805179377524352   Loss 0.11109493\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7750692520775623   Loss 0.59782434\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       970\n",
      "          1       0.75      0.77      0.76       835\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 217]\n",
      " [189 646]]\n",
      "\n",
      "\n",
      "Epoch 5100\n",
      "# Train Accuracy 0.9807555238774056   Loss 0.11048856\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.598915\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 216]\n",
      " [189 647]]\n",
      "\n",
      "\n",
      "Epoch 5150\n",
      "# Train Accuracy 0.9809931100023759   Loss 0.109889016\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7767313019390581   Loss 0.6000042\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[753 214]\n",
      " [189 649]]\n",
      "\n",
      "\n",
      "Epoch 5200\n",
      "# Train Accuracy 0.9817058683772868   Loss 0.10929626\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7761772853185596   Loss 0.60109174\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 214]\n",
      " [190 649]]\n",
      "\n",
      "\n",
      "Epoch 5250\n",
      "# Train Accuracy 0.9817058683772868   Loss 0.10871012\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.60217774\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 215]\n",
      " [190 648]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 5300\n",
      "# Train Accuracy 0.9821810406272273   Loss 0.10813047\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.60326207\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 215]\n",
      " [190 648]]\n",
      "\n",
      "\n",
      "Epoch 5350\n",
      "# Train Accuracy 0.9821810406272273   Loss 0.10755723\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.775623268698061   Loss 0.6043446\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1805\n",
      "\n",
      "[[752 215]\n",
      " [190 648]]\n",
      "\n",
      "\n",
      "Epoch 5400\n",
      "# Train Accuracy 0.9824186267521977   Loss 0.106990255\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.6054254\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 5450\n",
      "# Train Accuracy 0.9828937990021382   Loss 0.106429465\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.60650426\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 5500\n",
      "# Train Accuracy 0.9833689712520789   Loss 0.10587469\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.60758144\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 5550\n",
      "# Train Accuracy 0.9833689712520789   Loss 0.10532588\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7745152354570637   Loss 0.60865635\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.78      0.77      0.77      1805\n",
      "\n",
      "[[751 216]\n",
      " [191 647]]\n",
      "\n",
      "\n",
      "Epoch 5600\n",
      "# Train Accuracy 0.9833689712520789   Loss 0.10478292\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7739612188365651   Loss 0.6097295\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 216]\n",
      " [192 647]]\n",
      "\n",
      "\n",
      "Epoch 5650\n",
      "# Train Accuracy 0.9836065573770492   Loss 0.10424573\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7734072022160665   Loss 0.6108005\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 217]\n",
      " [192 646]]\n",
      "\n",
      "\n",
      "Epoch 5700\n",
      "# Train Accuracy 0.9838441435020194   Loss 0.1037142\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7734072022160665   Loss 0.6118695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 217]\n",
      " [192 646]]\n",
      "\n",
      "\n",
      "Epoch 5750\n",
      "# Train Accuracy 0.9840817296269898   Loss 0.10318818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7734072022160665   Loss 0.6129366\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 217]\n",
      " [192 646]]\n",
      "\n",
      "\n",
      "Epoch 5800\n",
      "# Train Accuracy 0.9840817296269898   Loss 0.10266764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7734072022160665   Loss 0.6140013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 217]\n",
      " [192 646]]\n",
      "\n",
      "\n",
      "Epoch 5850\n",
      "# Train Accuracy 0.9840817296269898   Loss 0.10215248\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7734072022160665   Loss 0.6150639\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.78      0.79       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 217]\n",
      " [192 646]]\n",
      "\n",
      "\n",
      "Epoch 5900\n",
      "# Train Accuracy 0.9847944880019007   Loss 0.1016426\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.61612433\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 5950\n",
      "# Train Accuracy 0.9847944880019007   Loss 0.10113793\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.61718243\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6000\n",
      "# Train Accuracy 0.9847944880019007   Loss 0.100638375\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.61823833\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 6050\n",
      "# Train Accuracy 0.9847944880019007   Loss 0.10014384\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.98      0.98      1989\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.61929196\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 6100\n",
      "# Train Accuracy 0.985032074126871   Loss 0.099654265\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2219\n",
      "          1       0.98      0.98      0.98      1990\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6203433\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 6150\n",
      "# Train Accuracy 0.9855072463768116   Loss 0.099169545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.6213924\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 6200\n",
      "# Train Accuracy 0.9855072463768116   Loss 0.09868963\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.6224391\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 6250\n",
      "# Train Accuracy 0.9855072463768116   Loss 0.098214425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.62348354\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 6300\n",
      "# Train Accuracy 0.9855072463768116   Loss 0.09774387\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6245254\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 218]\n",
      " [193 645]]\n",
      "\n",
      "\n",
      "Epoch 6350\n",
      "# Train Accuracy 0.9855072463768116   Loss 0.097277895\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.98      0.98      0.98      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.6255652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6400\n",
      "# Train Accuracy 0.9857448325017819   Loss 0.09681642\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.6266024\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6450\n",
      "# Train Accuracy 0.9857448325017819   Loss 0.09635943\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.6276373\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6500\n",
      "# Train Accuracy 0.9857448325017819   Loss 0.09590673\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.98      0.98      0.98      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.62866974\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6550\n",
      "# Train Accuracy 0.9859824186267522   Loss 0.09545836\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2219\n",
      "          1       0.98      0.99      0.99      1990\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.6296999\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6600\n",
      "# Train Accuracy 0.9862200047517224   Loss 0.095014244\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.63072747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6650\n",
      "# Train Accuracy 0.9862200047517224   Loss 0.09457428\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7728531855955678   Loss 0.6317526\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.79       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 218]\n",
      " [192 645]]\n",
      "\n",
      "\n",
      "Epoch 6700\n",
      "# Train Accuracy 0.9862200047517224   Loss 0.09413842\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6327755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 218]\n",
      " [193 645]]\n",
      "\n",
      "\n",
      "Epoch 6750\n",
      "# Train Accuracy 0.9862200047517224   Loss 0.09370666\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.63379574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 218]\n",
      " [193 645]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 6800\n",
      "# Train Accuracy 0.9862200047517224   Loss 0.09327887\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.63481367\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 218]\n",
      " [193 645]]\n",
      "\n",
      "\n",
      "Epoch 6850\n",
      "# Train Accuracy 0.9862200047517224   Loss 0.09285503\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.63582915\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 218]\n",
      " [193 645]]\n",
      "\n",
      "\n",
      "Epoch 6900\n",
      "# Train Accuracy 0.9862200047517224   Loss 0.09243503\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.98      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.63684213\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 218]\n",
      " [193 645]]\n",
      "\n",
      "\n",
      "Epoch 6950\n",
      "# Train Accuracy 0.9864575908766928   Loss 0.09201888\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2221\n",
      "          1       0.98      0.99      0.99      1988\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.63785285\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 218]\n",
      " [193 645]]\n",
      "\n",
      "\n",
      "Epoch 7000\n",
      "# Train Accuracy 0.9864575908766928   Loss 0.091606475\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2221\n",
      "          1       0.98      0.99      0.99      1988\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.6388609\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 7050\n",
      "# Train Accuracy 0.9864575908766928   Loss 0.091197796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2221\n",
      "          1       0.98      0.99      0.99      1988\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6398665\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 219]\n",
      " [194 644]]\n",
      "\n",
      "\n",
      "Epoch 7100\n",
      "# Train Accuracy 0.9864575908766928   Loss 0.09079277\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2221\n",
      "          1       0.98      0.99      0.99      1988\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6408697\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 219]\n",
      " [194 644]]\n",
      "\n",
      "\n",
      "Epoch 7150\n",
      "# Train Accuracy 0.9869327631266334   Loss 0.09039138\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2219\n",
      "          1       0.99      0.99      0.99      1990\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7706371191135734   Loss 0.6418704\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 220]\n",
      " [194 643]]\n",
      "\n",
      "\n",
      "Epoch 7200\n",
      "# Train Accuracy 0.987407935376574   Loss 0.08999352\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.6428687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       969\n",
      "          1       0.74      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 221]\n",
      " [194 642]]\n",
      "\n",
      "\n",
      "Epoch 7250\n",
      "# Train Accuracy 0.987407935376574   Loss 0.0895992\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.6438645\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       970\n",
      "          1       0.74      0.77      0.76       835\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 222]\n",
      " [194 641]]\n",
      "\n",
      "\n",
      "Epoch 7300\n",
      "# Train Accuracy 0.987407935376574   Loss 0.08920832\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.64485794\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       971\n",
      "          1       0.74      0.77      0.75       834\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 223]\n",
      " [194 640]]\n",
      "\n",
      "\n",
      "Epoch 7350\n",
      "# Train Accuracy 0.987407935376574   Loss 0.088820875\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.6458489\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       971\n",
      "          1       0.74      0.77      0.75       834\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 223]\n",
      " [194 640]]\n",
      "\n",
      "\n",
      "Epoch 7400\n",
      "# Train Accuracy 0.9878831076265147   Loss 0.08843675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.64683753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       971\n",
      "          1       0.74      0.77      0.75       834\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 223]\n",
      " [194 640]]\n",
      "\n",
      "\n",
      "Epoch 7450\n",
      "# Train Accuracy 0.9878831076265147   Loss 0.088055976\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7684210526315789   Loss 0.6478235\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       970\n",
      "          1       0.74      0.77      0.75       835\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 223]\n",
      " [195 640]]\n",
      "\n",
      "\n",
      "Epoch 7500\n",
      "# Train Accuracy 0.9878831076265147   Loss 0.0876785\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7684210526315789   Loss 0.6488069\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.74      0.77      0.75       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 222]\n",
      " [196 641]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 7550\n",
      "# Train Accuracy 0.9881206937514849   Loss 0.08730423\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.99      0.99      0.99      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.64978826\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.74      0.77      0.75       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 221]\n",
      " [196 642]]\n",
      "\n",
      "\n",
      "Epoch 7600\n",
      "# Train Accuracy 0.9881206937514849   Loss 0.086933166\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.99      0.99      0.99      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.650767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.74      0.77      0.75       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 221]\n",
      " [196 642]]\n",
      "\n",
      "\n",
      "Epoch 7650\n",
      "# Train Accuracy 0.9881206937514849   Loss 0.086565234\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.99      0.99      0.99      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.6517431\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.74      0.77      0.75       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 221]\n",
      " [196 642]]\n",
      "\n",
      "\n",
      "Epoch 7700\n",
      "# Train Accuracy 0.9883582798764552   Loss 0.08620043\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2219\n",
      "          1       0.99      0.99      0.99      1990\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.6527173\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.74      0.77      0.75       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 221]\n",
      " [196 642]]\n",
      "\n",
      "\n",
      "Epoch 7750\n",
      "# Train Accuracy 0.9888334521263958   Loss 0.085838675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7689750692520776   Loss 0.65368867\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.74      0.77      0.75       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 221]\n",
      " [196 642]]\n",
      "\n",
      "\n",
      "Epoch 7800\n",
      "# Train Accuracy 0.9890710382513661   Loss 0.08547994\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.6546579\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 220]\n",
      " [196 643]]\n",
      "\n",
      "\n",
      "Epoch 7850\n",
      "# Train Accuracy 0.9890710382513661   Loss 0.085124195\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.65562445\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 220]\n",
      " [195 643]]\n",
      "\n",
      "\n",
      "Epoch 7900\n",
      "# Train Accuracy 0.9890710382513661   Loss 0.084771425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.6565886\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 220]\n",
      " [196 643]]\n",
      "\n",
      "\n",
      "Epoch 7950\n",
      "# Train Accuracy 0.9890710382513661   Loss 0.084421515\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.6575505\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 220]\n",
      " [195 643]]\n",
      "\n",
      "\n",
      "Epoch 8000\n",
      "# Train Accuracy 0.9890710382513661   Loss 0.084074505\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.65851\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 220]\n",
      " [195 643]]\n",
      "\n",
      "\n",
      "Epoch 8050\n",
      "# Train Accuracy 0.9893086243763364   Loss 0.08373034\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.65946716\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 220]\n",
      " [195 643]]\n",
      "\n",
      "\n",
      "Epoch 8100\n",
      "# Train Accuracy 0.989783796626277   Loss 0.08338894\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.6604219\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 220]\n",
      " [195 643]]\n",
      "\n",
      "\n",
      "Epoch 8150\n",
      "# Train Accuracy 0.989783796626277   Loss 0.08305034\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.6613741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.74      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 221]\n",
      " [195 642]]\n",
      "\n",
      "\n",
      "Epoch 8200\n",
      "# Train Accuracy 0.989783796626277   Loss 0.08271444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.662324\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.74      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 221]\n",
      " [195 642]]\n",
      "\n",
      "\n",
      "Epoch 8250\n",
      "# Train Accuracy 0.9902589688762177   Loss 0.08238125\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.6632715\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 220]\n",
      " [195 643]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 8300\n",
      "# Train Accuracy 0.9904965550011879   Loss 0.08205075\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7706371191135734   Loss 0.6642168\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 220]\n",
      " [194 643]]\n",
      "\n",
      "\n",
      "Epoch 8350\n",
      "# Train Accuracy 0.9907341411261582   Loss 0.081722856\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7706371191135734   Loss 0.6651596\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 220]\n",
      " [194 643]]\n",
      "\n",
      "\n",
      "Epoch 8400\n",
      "# Train Accuracy 0.9909717272511286   Loss 0.08139754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7706371191135734   Loss 0.6661001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 220]\n",
      " [194 643]]\n",
      "\n",
      "\n",
      "Epoch 8450\n",
      "# Train Accuracy 0.9909717272511286   Loss 0.08107481\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2216\n",
      "          1       0.99      0.99      0.99      1993\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6670383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 219]\n",
      " [194 644]]\n",
      "\n",
      "\n",
      "Epoch 8500\n",
      "# Train Accuracy 0.9912093133760989   Loss 0.08075461\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.6679741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 8550\n",
      "# Train Accuracy 0.9912093133760989   Loss 0.08043693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.6689077\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 8600\n",
      "# Train Accuracy 0.9914468995010691   Loss 0.08012171\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2214\n",
      "          1       0.99      0.99      0.99      1995\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.6698388\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 8650\n",
      "# Train Accuracy 0.9919220717510098   Loss 0.07980894\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2214\n",
      "          1       0.99      0.99      0.99      1995\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.67076766\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 219]\n",
      " [193 644]]\n",
      "\n",
      "\n",
      "Epoch 8700\n",
      "# Train Accuracy 0.99215965787598   Loss 0.07949856\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2213\n",
      "          1       0.99      0.99      0.99      1996\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6716942\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 8750\n",
      "# Train Accuracy 0.99215965787598   Loss 0.07919058\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2213\n",
      "          1       0.99      0.99      0.99      1996\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6726185\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 8800\n",
      "# Train Accuracy 0.9923972440009503   Loss 0.078884944\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2214\n",
      "          1       0.99      0.99      0.99      1995\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6735404\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 8850\n",
      "# Train Accuracy 0.9923972440009503   Loss 0.07858167\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2214\n",
      "          1       0.99      0.99      0.99      1995\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6744602\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 8900\n",
      "# Train Accuracy 0.9926348301259207   Loss 0.07828067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2215\n",
      "          1       0.99      0.99      0.99      1994\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.67537767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 8950\n",
      "# Train Accuracy 0.9931100023758612   Loss 0.07798198\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6762927\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 9000\n",
      "# Train Accuracy 0.9931100023758612   Loss 0.0776855\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.67720574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 9050\n",
      "# Train Accuracy 0.9931100023758612   Loss 0.07739126\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6781163\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9100\n",
      "# Train Accuracy 0.9931100023758612   Loss 0.07709922\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.67902476\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9150\n",
      "# Train Accuracy 0.9931100023758612   Loss 0.07680937\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6799309\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9200\n",
      "# Train Accuracy 0.9931100023758612   Loss 0.076521635\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2217\n",
      "          1       0.99      0.99      0.99      1992\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6808349\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9250\n",
      "# Train Accuracy 0.9933475885008316   Loss 0.076236024\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.99      0.99      0.99      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6817366\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9300\n",
      "# Train Accuracy 0.9933475885008316   Loss 0.07595253\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2218\n",
      "          1       0.99      0.99      0.99      1991\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.68263614\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9350\n",
      "# Train Accuracy 0.9935851746258019   Loss 0.07567112\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2219\n",
      "          1       0.99      0.99      0.99      1990\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6835333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9400\n",
      "# Train Accuracy 0.9938227607507721   Loss 0.07539174\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.99      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.6844285\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9450\n",
      "# Train Accuracy 0.9938227607507721   Loss 0.07511439\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.99      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7722991689750692   Loss 0.68532133\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 219]\n",
      " [192 644]]\n",
      "\n",
      "\n",
      "Epoch 9500\n",
      "# Train Accuracy 0.9938227607507721   Loss 0.07483906\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      2220\n",
      "          1       0.99      0.99      0.99      1989\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.7717451523545706   Loss 0.686212\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       970\n",
      "          1       0.75      0.77      0.76       835\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[750 220]\n",
      " [192 643]]\n",
      "\n",
      "\n",
      "Epoch 9550\n",
      "# Train Accuracy 0.994535519125683   Loss 0.074565716\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99      2221\n",
      "          1       0.99      0.99      0.99      1988\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6871005\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 9600\n",
      "# Train Accuracy 0.9947731052506533   Loss 0.07429432\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2222\n",
      "          1       0.99      1.00      0.99      1987\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6879869\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 9650\n",
      "# Train Accuracy 0.9947731052506533   Loss 0.07402488\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2222\n",
      "          1       0.99      1.00      0.99      1987\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.688871\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 9700\n",
      "# Train Accuracy 0.9950106913756237   Loss 0.07375734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2223\n",
      "          1       0.99      1.00      0.99      1986\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4209\n",
      "\n",
      "# Test Accuracy 0.771191135734072   Loss 0.6897531\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.77      0.78       969\n",
      "          1       0.75      0.77      0.76       836\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[749 220]\n",
      " [193 643]]\n",
      "\n",
      "\n",
      "Epoch 9750\n",
      "# Train Accuracy 0.9950106913756237   Loss 0.07349173\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2223\n",
      "          1       0.99      1.00      0.99      1986\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4209\n",
      "\n",
      "# Test Accuracy 0.7706371191135734   Loss 0.6906332\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       968\n",
      "          1       0.75      0.77      0.76       837\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[748 220]\n",
      " [194 643]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 9800\n",
      "# Train Accuracy 0.9950106913756237   Loss 0.07322799\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2223\n",
      "          1       0.99      1.00      0.99      1986\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4209\n",
      "\n",
      "# Test Accuracy 0.7700831024930748   Loss 0.69151086\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       967\n",
      "          1       0.75      0.77      0.76       838\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[747 220]\n",
      " [195 643]]\n",
      "\n",
      "\n",
      "Epoch 9850\n",
      "# Train Accuracy 0.9950106913756237   Loss 0.07296612\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2223\n",
      "          1       0.99      1.00      0.99      1986\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.69238645\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 220]\n",
      " [196 643]]\n",
      "\n",
      "\n",
      "Epoch 9900\n",
      "# Train Accuracy 0.995248277500594   Loss 0.07270605\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2224\n",
      "          1       0.99      1.00      0.99      1985\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.6932601\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 220]\n",
      " [196 643]]\n",
      "\n",
      "\n",
      "Epoch 9950\n",
      "# Train Accuracy 0.995248277500594   Loss 0.0724478\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00      2224\n",
      "          1       0.99      1.00      0.99      1985\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4209\n",
      "\n",
      "# Test Accuracy 0.7695290858725762   Loss 0.69413143\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.77      0.78       966\n",
      "          1       0.75      0.77      0.76       839\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1805\n",
      "\n",
      "[[746 220]\n",
      " [196 643]]\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    steps = 10000\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    for epochs in range(steps):\n",
    "        \n",
    "        pred_y, _, train_loss = sess.run([y_pred, train_step,cross_entropy],feed_dict={x:train_dat,y_true:train_label_one_hot})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 50 STEPS\n",
    "        if epochs%50 == 0:\n",
    "            print(\"\\n\\nEpoch\", epochs)\n",
    "\n",
    "            train_accuracy = np.mean(np.equal(np.argmax(pred_y, axis = 1), np.argmax(train_label_one_hot, axis = 1).flatten()))\n",
    "            print(\"# Train Accuracy\", train_accuracy, \"  Loss\", train_loss)\n",
    "            print(classification_report(np.argmax(pred_y, axis = 1),np.argmax(train_label_one_hot, axis = 1)))\n",
    "            \n",
    "            #save the accuracy for training\n",
    "            train_acc_history.append(train_accuracy)\n",
    "            train_loss_history.append(train_loss)\n",
    "            \n",
    "            # Predictions validation\n",
    "            pred_y_test = sess.run(y_pred,feed_dict={x:test_dat,y_true:test_label_one_hot})\n",
    "            \n",
    "            test_loss = sess.run(test_entropy, feed_dict={predicted_test:pred_y_test,test_true:test_label_one_hot})\n",
    "            test_loss_history.append(test_loss)\n",
    "            \n",
    "            pred_y_test_val = np.argmax(pred_y_test, axis = 1)\n",
    "            test_label_val = np.argmax(test_label_one_hot, axis = 1)\n",
    "            \n",
    "            test_accuracy = np.mean(np.equal(pred_y_test_val, test_label_val.flatten()))\n",
    "            print(\"# Test Accuracy\", test_accuracy, \"  Loss\", test_loss)\n",
    "            print(classification_report(pred_y_test_val, test_label_val))\n",
    " \n",
    "            print(confusion_matrix(pred_y_test_val, test_label_val))\n",
    "\n",
    "            test_acc_history.append(test_accuracy)\n",
    "            \n",
    "        #break\n",
    " \n",
    "    #once we done all the training and testing, save the trained model for future use\n",
    "    saver.save(sess, './saved_model_alexnet/Alex_DNN.ckpt') \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a400ae5f8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2a822208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(1, len(train_acc_history), len(train_acc_history))\n",
    "x *= 50\n",
    "plt.plot(x, train_acc_history, label = 'training')\n",
    "plt.plot(x, test_acc_history, label = 'testing')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Training VS Testing Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a40159d68>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2a837400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, train_loss_history, label = 'training')\n",
    "plt.plot(x, test_loss_history, label = 'testing')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training VS Testing Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789473684211\n",
      "0.517764\n"
     ]
    }
   ],
   "source": [
    "print(np.array(test_acc_history).max())\n",
    "print(np.array(test_loss_history).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
