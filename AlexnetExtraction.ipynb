{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Alexnet Graph and load the pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from numpy import *\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w, padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "        https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i % group == 0\n",
    "    assert c_o % group == 0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    if group == 1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups = tf.split(input, group, 3)  # tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(kernel, group, 3)  # tf.split(3, group, kernel)\n",
    "        output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(output_groups, 3)  # tf.concat(3, output_groups)\n",
    "    return tf.reshape(tf.nn.bias_add(conv, biases), [-1] + conv.get_shape().as_list()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.63 s, sys: 10.2 s, total: 19.9 s\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_x = zeros((1, 227,227,3)).astype(float32)\n",
    "train_y = zeros((1, 1000))\n",
    "xdim = train_x.shape[1:]\n",
    "ydim = train_y.shape[1]\n",
    "\n",
    "net_data = load(open(\"weights/bvlc_alexnet.npy\", \"rb\"), encoding=\"latin1\").item()\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "\n",
    "\n",
    "#conv1\n",
    "#conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "conv1_in = conv(x, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "#lrn1\n",
    "#lrn(2, 2e-05, 0.75, name='norm1')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                                  depth_radius=radius,\n",
    "                                                  alpha=alpha,\n",
    "                                                  beta=beta,\n",
    "                                                  bias=bias)\n",
    "\n",
    "#maxpool1\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "\n",
    "#conv2\n",
    "#conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv2 = tf.nn.relu(conv2_in)\n",
    "\n",
    "\n",
    "#lrn2\n",
    "#lrn(2, 2e-05, 0.75, name='norm2')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                                  depth_radius=radius,\n",
    "                                                  alpha=alpha,\n",
    "                                                  beta=beta,\n",
    "                                                  bias=bias)\n",
    "\n",
    "#maxpool2\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "#conv3\n",
    "#conv(3, 3, 384, 1, 1, name='conv3')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "#conv4\n",
    "#conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "\n",
    "#conv5\n",
    "#conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv5 = tf.nn.relu(conv5_in)\n",
    "\n",
    "#maxpool5\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "#fc6\n",
    "#fc(4096, name='fc6')\n",
    "fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "\n",
    "#fc7\n",
    "#fc(4096, name='fc7')\n",
    "fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n",
    "\n",
    "#fc8\n",
    "#fc(1000, relu=False, name='fc8')\n",
    "fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
    "fc8b = tf.Variable(net_data[\"fc8\"][1])\n",
    "fc8 = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "\n",
    "#prob\n",
    "#softmax(name='prob'))\n",
    "prob = tf.nn.softmax(fc8)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the car_url_id to perform the extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('balanced_cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_url_id</th>\n",
       "      <th>is_premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-xuefolan-lechi-d77BsJBbW9uE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-fengtian-weichi-dIIJBBIhZZWZ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          car_url_id  is_premium\n",
       "0   2010-xuefolan-lechi-d77BsJBbW9uE       False\n",
       "1  2009-fengtian-weichi-dIIJBBIhZZWZ       False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3007"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base on the car_url_id, read the images and augmented images\n",
    "### And convert the images into extracted features and store in .txt / .txt_ format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0\n",
      "processing 50\n",
      "processing 100\n",
      "processing 150\n",
      "processing 200\n",
      "processing 250\n",
      "processing 300\n",
      "processing 350\n",
      "processing 400\n",
      "processing 450\n",
      "processing 500\n",
      "processing 550\n",
      "processing 600\n",
      "processing 650\n",
      "processing 700\n",
      "processing 750\n",
      "processing 800\n",
      "processing 850\n",
      "processing 900\n",
      "processing 950\n",
      "processing 1000\n",
      "processing 1050\n",
      "processing 1100\n",
      "processing 1150\n",
      "processing 1200\n",
      "processing 1250\n",
      "processing 1300\n",
      "processing 1350\n",
      "processing 1400\n",
      "processing 1450\n",
      "processing 1500\n",
      "processing 1550\n",
      "processing 1600\n",
      "processing 1650\n",
      "processing 1700\n",
      "processing 1750\n",
      "processing 1800\n",
      "processing 1850\n",
      "processing 1900\n",
      "processing 1950\n",
      "processing 2000\n",
      "processing 2050\n",
      "processing 2100\n",
      "processing 2150\n",
      "processing 2200\n",
      "processing 2250\n",
      "processing 2300\n",
      "processing 2350\n",
      "processing 2400\n",
      "processing 2450\n",
      "processing 2500\n",
      "processing 2550\n",
      "processing 2600\n",
      "processing 2650\n",
      "processing 2700\n",
      "processing 2750\n",
      "processing 2800\n",
      "processing 2850\n",
      "processing 2900\n",
      "processing 2950\n",
      "processing 3000\n",
      "CPU times: user 13min 39s, sys: 1min 26s, total: 15min 5s\n",
      "Wall time: 7min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#alexnet expected input is 227 * 227 *3\n",
    "process_dimension = 227\n",
    "for idx in range(len(df)):\n",
    "#for idx in range(2):\n",
    " \n",
    "    if idx % 50 == 0:\n",
    "        print ('processing {}'.format(idx))\n",
    "\n",
    "    car_url_id = df.loc[idx, 'car_url_id']\n",
    "    image_file_name = '{}'.format(car_url_id)\n",
    "\n",
    "    exist = glob.glob('extracted_alex/{}.txt'.format(car_url_id))\n",
    "    if(len(exist)>0):\n",
    "        print('this file already processed: {}, skip to next.'.format(image_file_name))\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        #read the image file and resize it to 227 x 227\n",
    "        img1 = cv2.imread('aug_img2/{}.png'.format(image_file_name))\n",
    "        img2 = cv2.imread('aug_img2/{}_aug.png'.format(image_file_name))\n",
    "        \n",
    "        #print(img1.shape, img2.shape)\n",
    "    except:\n",
    "        print('img not found :', image_file_name, ' : ',img)\n",
    "    fc7_ = sess.run(fc7, feed_dict={x: [img1]})\n",
    "    np.savetxt('extracted_alex/{}.txt'.format(car_url_id), fc7_)\n",
    "    \n",
    "    fc7_ = sess.run(fc7, feed_dict={x: [img2]})\n",
    "    np.savetxt('extracted_alex/{}_aug.txt'.format(car_url_id), fc7_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
